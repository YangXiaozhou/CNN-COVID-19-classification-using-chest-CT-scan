{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extend the ideas developed in the previous notebook to build a neural network using functions;\n",
    "- Implement functions for the initialization of, forward propagation through, and updating of a neural network;\n",
    "- Apply the logic of backpropagation in more detail;\n",
    "- Hand-code gradient descent using `numpy` for a more realistic size of problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having spent spent some time working on the ideas of supervised learning and getting familiar with the terminology of neural networks, let's write some code to implement a neural network from scratch. We're going to use a functional programming style to help build intuition. To make matters easier, we'll use a dictionary called `model` to store all data associated with the neural network (the weight matrices, the  bias vectors, etc.) and pass that into functions as a single argument. We'll also assume that the activation functions are the same in all the layers (i.e., the *logistic* or *sigmoid* function) to simplify the implementation. Production codes usually use an object-oriented style to build networks and, of course, are optimized for efficiency (unlike what we'll develop here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to borrow notation from Michael Neilsen's [*Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com) to make life easier. In particular, we'll let $W^\\ell$ and $b^\\ell$ denote the weight matrices & bias vectors respectively associated with the $\\ell$th layer of the network. The entry $W^{\\ell}_{jk}$ of $W^\\ell$ is the weight parameter associated with the link connecting the $k$th neuron in layer $\\ell-1$ to the $j$th neuron in layer $\\ell$:\n",
    "\n",
    "[![](../img/tikz16.png)](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "\n",
    "Let's put this altogether now and construct a network from scratch. We start with some typical imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create an initialization function to set up model\n",
    "\n",
    "Rather than the fixed constants in `setup` from before, write a function `initialize_model` that accepts a list  `dimensions` of positive integer inputs that constructs a `dict` with specific key-value pairs:\n",
    "+ `model['nlayers']` : number of layers in neural network\n",
    "+ `model['weights']` : list of NumPy matrices with appropriate dimensions\n",
    "+ `model['biases']` : list of NumPy (column) vectors of appropriate dimensions\n",
    "+ The matrices in `model['weights']` and the vectors in `model['biases']` should be initialized as randomly arrays of the appropriate shapes.\n",
    "\n",
    "If the input list `dimensions` has `L+1` entries, the number of layers is `L` (the first entry of `dimensions` is the input dimension, the next ones are the number of units/neurons in each subsequent layer going up to the output layer).\n",
    "Thus, for example:\n",
    "\n",
    "```python\n",
    ">>> dimensions = [784, 15, 10]\n",
    ">>> model = initialize_model(dimensions)\n",
    ">>> for k, (W, b) in enumerate(zip(model['weights'], model['biases'])):\n",
    ">>>    print(f'Layer {k+1}:\\tShape of W{k+1}: {W.shape}\\tShape of b{k+1}: {b.shape}')\n",
    "```\n",
    "```\n",
    "Layer 1:\tShape of W1: (15, 784)\tShape of b1: (15, 1)\n",
    "Layer 2:\tShape of W2: (10, 15)\tShape of b2: (10, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(dimensions):\n",
    "    '''Accepts a list of positive integers; returns a dict 'model' with key/values as follows:\n",
    "      model['nlayers'] : number of layers in neural network\n",
    "      model['weights'] : list of NumPy matrices with appropriate dimensions\n",
    "      model['biases'] : list of NumPy (column) vectors of appropriate dimensions\n",
    "    These correspond to the weight matrices & bias vectors associated with each layer of a neural network.'''\n",
    "    weights, biases = [],[]\n",
    "    L = len(dimensions) - 1\n",
    "    for l in range(L):\n",
    "        W = np.random.normal(size=(dimensions[l+1], dimensions[l]))\n",
    "        b = np.random.normal(size=(dimensions[l+1],1))\n",
    "        \n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    \n",
    "    return dict(weights=weights, biases=biases, nlayers=L) # Should return a dict as described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:\tShape of W1: (15, 784)\tShape of b1: (15, 1)\n",
      "Layer 2:\tShape of W2: (10, 15)\tShape of b2: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "dimensions = [784, 15, 10]\n",
    "model = initialize_model(dimensions)\n",
    "for k, (W, b) in enumerate(zip(model['weights'], model['biases'])):\n",
    "    print(f'Layer {k+1}:\\tShape of W{k+1}: {W.shape}\\tShape of b{k+1}: {b.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement activation function(s), loss functions, & their derivatives\n",
    "For today's purposes, we'll use only the *logistic* or *sigmoid* function as an activation function:\n",
    "$$ \\sigma(x) = \\frac{1}{1+\\exp(-x)} = \\frac{\\exp(x)}{1+\\exp(x)}.$$\n",
    "A bit of calculus shows that\n",
    "$$ \\sigma'(x) = \\sigma(x)(1-\\sigma(x)) .$$\n",
    "\n",
    "Actually, a more numerically robust formula for $\\sigma(x)$ (i.e., one that works for large positive or large negative input equally well) is\n",
    "$$\n",
    "\\sigma(x) = \\begin{cases} \\frac{1}{1+\\exp(-x)} & (x\\ge0) \\\\ 1 - \\frac{1}{1+\\exp(x)} & \\mathrm{otherwise} \\end{cases}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loss function, we'll use the typical \"$L_2$-norm of the error\" (alternatively called *mean-square error (MSE)* when averaged over a batch of values:\n",
    "$$ \\mathcal{E}(\\hat{y},y) = \\frac{1}{2} \\|\\hat{y}-y\\|^{2} = \\frac{1}{2} \\sum_{k=1}^{d} \\left[ \\hat{y}_{k}-y_{k} \\right]^{2}.$$\n",
    "Again, using multivariable calculus, we can see that\n",
    "$$\\nabla_{\\hat{y}} \\mathcal{E}(\\hat{y},y) = \\hat{y} - y.$$\n",
    "\n",
    "Implement all four of these functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    '''The logistic function; accepts arbitrary arrays as input (vectorized)'''\n",
    "    return np.where(x>=0, 1/(1+np.exp(-x)), 1 - 1/(1+np.exp(x))) # piecewise for numerical robustness\n",
    "\n",
    "def sigma_prime(x):\n",
    "    '''The *derivative* of the logistic function; accepts arbitrary arrays as input (vectorized)'''\n",
    "    return sigma(x)*(1-sigma(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(yhat, y):\n",
    "    '''The loss as measured by the L2-norm squared of the error'''\n",
    "    return 0.5*np.square(yhat-y).sum()\n",
    "    \n",
    "def loss_prime(yhat, y):\n",
    "    '''Implementation of the gradient of the loss function'''\n",
    "    return (yhat-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement a function for forward propagation\n",
    "\n",
    "Write a function `forward` that uses the architecture described in a `dict` as created by `initialize_model` to evaluate the output of the neural network for a given input *column* vector `x`.\n",
    "+ Take $a^{0}=x$ from the input.\n",
    "+ For $\\ell=1,\\dotsc,L$, compute & store the intermediate computed vectors $z^{\\ell}=W^{\\ell}a^{\\ell-1}+b^{\\ell}$ (the *weighted inputs*) and $a^{\\ell}=\\sigma\\left(z^{\\ell}\\right)$ (the *activations*) in an updated dictionary `model`. That is, modify the input dictionary `model` so as to accumulate:\n",
    "  + `model['activations']`: a list with entries $a^{\\ell}$ for $\\ell=0,\\dotsc,L$\n",
    "  + `model['z_inputs']`: a list with entries $z^{\\ell}$ for $\\ell=1,\\dotsc,L$\n",
    "+ The function should return the computed output $a^{L}$ and the modified dictionary `model`.\n",
    "Notice that input `z` can be a matrix of dimension $n_{0} \\times N_{\\mathrm{batch}}$ corresponding to a batch of input vectors (here, $n_0$ is the dimension of the expected input vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract process into function and run tests again.\n",
    "def forward(x, model):\n",
    "    '''Implementation of forward propagation through a feed-forward neural network.\n",
    "       x : input array oriented column-wise (i.e., features along the rows)\n",
    "       model : dict with same keys as output of initialize_model & appropriate lists in 'weights' & 'biases'\n",
    "    The output dict model is the same as the input with additional keys 'z_inputs' & 'activations';\n",
    "    these are accumulated to be used later for backpropagation. Notice the lists model['z_inputs'] &\n",
    "    model['activations'] both have the same number of entries as model['weights'] & model['biases']\n",
    "    (one for each layer).\n",
    "    '''\n",
    "    a = x\n",
    "    activations = [a]\n",
    "    zs = []\n",
    "    for W,b in zip(model['weights'], model['biases']):\n",
    "        z = W @ a + b\n",
    "        a = sigma(z)\n",
    "        zs.append(z)\n",
    "        activations.append(a)\n",
    "    model['activations'], model['z_inputs'] = activations, zs\n",
    "    return (a, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 15, 10]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before executing *forward*:\n",
      "keys == dict_keys(['weights', 'biases', 'nlayers'])\n"
     ]
    }
   ],
   "source": [
    "print(f'Before executing *forward*:\\nkeys == {model.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_batch = 3  # Let's use, say, 3 random inputs & their corresponding outputs\n",
    "x_input = np.random.rand(dimensions[0], N_batch)\n",
    "y = np.random.rand(dimensions[-1], N_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After executing *forward*:\n",
      "keys == dict_keys(['weights', 'biases', 'nlayers', 'activations', 'z_inputs'])\n"
     ]
    }
   ],
   "source": [
    "y_hat, model = forward(x_input, model)  # the dict model is *updated* by forward propagation\n",
    "print(f'After executing *forward*:\\nkeys == {model.keys()}')\n",
    "# Observe additional dict keys: 'activations' & 'z_inputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm for backpropagation:\n",
    "\n",
    "#### (optional reading for the mathematically brave)\n",
    "\n",
    "The description here is based on the *wonderfully concise* description from Michael Neilsen's [*Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com/chap2.html). Neilsen has artfully crafted a summary using the bare minimum mathematical prerequisites. The notation elegantly summarises the important ideas in a way to make implementation easy in array-based frameworks like Matlab or NumPy. This is the best description I (Dhavide) know of that does this.\n",
    "\n",
    "In the following, $\\mathcal{E}$ is the loss function and the symbol $\\odot$ is the [*Hadamard product*](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) of two conforming arrays; this is simply a fancy way of writing the usual element-wise product of arrays as computed by NumPy & is sometimes called the *Schur product*. This can be reformulated in usual matrix algebra for analysis.\n",
    "\n",
    "Given a neural network with $L$ layers (not including the \"input layer\") described by an appropriate architecture:\n",
    "\n",
    "1. Input $x$: Set the corresponding activation $a^{0} \\leftarrow x$ for the input layer.\n",
    "2. Feedforward: For each $\\ell=1,2,\\dotsc,L$, compute *weighted inputs* $z^{\\ell}$ & *activations* $a^{\\ell}$ using the formulas\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z^{\\ell} & \\leftarrow  W^{\\ell} a^{\\ell-1} + b^{\\ell}, \\\\\n",
    "a^{\\ell} & \\leftarrow  \\sigma\\left( z^{\\ell}\\right)\n",
    "\\end{aligned}.\n",
    "$$\n",
    "3. Starting from the end, compute the \"error\" in the output layer $\\delta^{L}$ according to the formula\n",
    "$$\n",
    "\\delta^{L} \\leftarrow \\nabla_{a^{L}} \\mathcal{E} \\odot \\sigma'\\left(z^{L}\\right)\n",
    "$$\n",
    "\n",
    "4. *Backpropagate* the \"error\" for $\\ell=Lâˆ’1\\dotsc,1$ using the formula\n",
    "$$\n",
    "\\delta^{\\ell} \\leftarrow \\left[ W^{\\ell+1}\\right]^{T}\\delta^{\\ell+1} \\odot \\sigma'\\left(z^{\\ell}\\right).\n",
    "$$\n",
    "5. The required gradients of the loss function $\\mathcal{E}$ with respect to the parameters $W^{\\ell}_{p,q}$ and $b^{\\ell}_{r}$ can be computed directly from the \"errors\" $\\left\\{ \\delta^{\\ell} \\right\\}$ and the weighted inputs $\\left\\{ z^{\\ell} \\right\\}$ according to the relations\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\frac{\\partial \\mathcal{E}}{\\partial W^{\\ell}_{p,q}} &= a^{\\ell-1}_{q} \\delta^{\\ell}_{p} &&(\\ell=1,\\dotsc,L)\\\\\n",
    "   \\frac{\\partial \\mathcal{E}}{\\partial b^{\\ell}_{r}} &= \\delta^{\\ell}_{r} &&\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement a function for backward propagation\n",
    "\n",
    "**This one is a freebie!**\n",
    "\n",
    "Implement a function `backward` that implements the back-propagation algorithm to compute the gradients of the loss function $\\mathcal{E}$ with respect to the weight matrices $W^{\\ell}$ and the bias vectors $b^{\\ell}$.\n",
    "+ The function should accept a column vector `y` of output labels and an appropriate dictionary `model` as input.\n",
    "+ The dict `model` is assumed to have been generated *after* a call to `forward`; that is, `model` should have keys `'w_inputs'` and `'activations'` as computed by a call to `forward`.\n",
    "+ The result will be a modified dictionary `model` with two additional key-value pairs:\n",
    "  + `model['grad_weights']`: a list with entries $\\nabla_{W^{\\ell}} \\mathcal{E}$ for $\\ell=1,\\dotsc,L$\n",
    "  + `model['grad_biases']`: a list with entries $\\nabla_{b^{\\ell}} \\mathcal{E}$ for $\\ell=1,\\dotsc,L$\n",
    "+ Notice the dimensions of the matrices $\\nabla_{W^{\\ell}} \\mathcal{E}$ and the vectors $\\nabla_{b^{\\ell}} \\mathcal{E}$ will be identical to those of ${W^{\\ell}}$ and ${b^{\\ell}}$ respectively.\n",
    "+ The function's return value is the modified dictionary `model`.\n",
    "\n",
    "\n",
    "We've done this for you (in the interest of time). Notice that input `y` can be a matrix of dimension $n_{L} \\times N_{\\mathrm{batch}}$ corresponding to a batch of output vectors (here, $n_L$ is the number of units in the output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(y, model):\n",
    "    '''Implementation of backward propagation of data through the network\n",
    "       y : output array oriented column-wise (i.e., features along the rows) as output by forward\n",
    "       model : dict with same keys as output by forward\n",
    "    Note the input needs to have keys 'nlayers', 'weights', 'biases', 'z_inputs', and 'activations'\n",
    "    '''\n",
    "    Nbatch = y.shape[1] # Needed to extend for batches of vectors\n",
    "    \n",
    "    # Compute the \"error\" delta^L for the output layer\n",
    "    yhat = model['activations'][-1]\n",
    "    z, a = model['z_inputs'][-1], model['activations'][-2]\n",
    "    delta = loss_prime(yhat, y) * sigma_prime(z)\n",
    "    \n",
    "    # Use delta^L to compute gradients w.r.t b & W in the output layer.\n",
    "    grad_b, grad_W = delta @ np.ones((Nbatch, 1)), np.dot(delta, a.T)\n",
    "    grad_weights, grad_biases = [grad_W], [grad_b]\n",
    "    loop_iterates = zip(model['weights'][-1:0:-1],\n",
    "                        model['z_inputs'][-2::-1],\n",
    "                        model['activations'][-3::-1])\n",
    "    \n",
    "    for W, z, a in loop_iterates:\n",
    "        delta = np.dot(W.T, delta) * sigma_prime(z)\n",
    "        grad_b, grad_W = delta @ np.ones((Nbatch, 1)), np.dot(delta, a.T)\n",
    "        grad_weights.append(grad_W)\n",
    "        grad_biases.append(grad_b)\n",
    "        \n",
    "    # We built up lists of gradients backwards, so we reverse the lists\n",
    "    model['grad_weights'], model['grad_biases'] = grad_weights[::-1], grad_biases[::-1]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before executing *backward*:\n",
      "keys == dict_keys(['weights', 'biases', 'nlayers', 'activations', 'z_inputs'])\n"
     ]
    }
   ],
   "source": [
    "# Use the test example from above. Assume model, x_input have been initialized & *forward* has been executed already.\n",
    "print(f'Before executing *backward*:\\nkeys == {model.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After executing *backward*:\n",
      "keys == dict_keys(['weights', 'biases', 'nlayers', 'activations', 'z_inputs', 'grad_weights', 'grad_biases'])\n"
     ]
    }
   ],
   "source": [
    "model = backward(y, model)  # the dict model is updated *again* by backward propagation\n",
    "print(f'After executing *backward*:\\nkeys == {model.keys()}')\n",
    "# Observe additional dict keys: 'grad_weights' & 'grad_biases'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement a function to update the model parameters using computed gradients.\n",
    "\n",
    "Given some positive learning rate $\\eta>0$, we want to change all the weights and biases using their gradients.\n",
    "Write a function `update` to compute a single step of gradient descent assuming that the model gradients have been computed for a given input vector.\n",
    "+ The functions signature should be `update(eta, model)` where `eta` is a positive scalar value and `model` is a dictionary as output from `backward`.\n",
    "+ The result will be an updated model with the values updated for `model['weights']` and `model['biases']`.\n",
    "+ Written using array notations, these updates can be expressed as\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   W^{\\ell} &\\leftarrow W^{\\ell} - \\eta \\nabla_{W^{\\ell}} \\mathcal{E} &&(\\ell=1,\\dotsc,L)\\\\\n",
    "   b^{\\ell} &\\leftarrow b^{\\ell} - \\eta \\nabla_{b^{\\ell}} \\mathcal{E} &&\n",
    "   \\end{aligned}.\n",
    "   $$\n",
    "+ Written out component-wise, the preceding array expressions would be written as\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "      W^{\\ell}_{p,q} &\\leftarrow W^{\\ell}_{p,q} - \\eta \\frac{\\partial \\mathcal{E}}{\\partial W^{\\ell}_{p,q}}\n",
    "      &&(\\ell=1,\\dotsc,L)\\\\\n",
    "      b^{\\ell}_{r} &\\leftarrow b^{\\ell}_{r} - \\eta \\frac{\\partial \\mathcal{E}}{\\partial b^{\\ell}_{r}} &&\n",
    "   \\end{aligned}\n",
    "   $$.\n",
    "+ For safety, have the update step delete the keys added by calls to `forward` and `backward`, i.e., the keys `'z_inputs'`, `'activations'`, `'grad_weights'`, & `'grad_biases'`.\n",
    "+ The output should be a dict `model` like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(eta, model):\n",
    "    '''Use learning rate and gradients to update model parameters\n",
    "       eta : learning rate (positive scalar parameter)\n",
    "       model : dict with same keys as output by backward\n",
    "    Output result is a modified dict model\n",
    "    '''\n",
    "    new_weights, new_biases = [], []\n",
    "    for W, b, dW, db in zip(model['weights'], model['biases'], model['grad_weights'], model['grad_biases']):\n",
    "        new_weights.append(W - eta * dW)\n",
    "        new_biases.append(b - eta * db)\n",
    "    model['weights'] = new_weights\n",
    "    model['biases'] = new_biases\n",
    "    # Get rid of extraneous keys/values\n",
    "    for key in ['z_inputs', 'activations', 'grad_weights', 'grad_biases']:\n",
    "        del model[key]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before executing *update*:\n",
      "keys == dict_keys(['weights', 'biases', 'nlayers', 'activations', 'z_inputs', 'grad_weights', 'grad_biases'])\n"
     ]
    }
   ],
   "source": [
    "# Use the test example from above. Assume *forward* & *backward* have been executed already.\n",
    "print(f'Before executing *update*:\\nkeys == {model.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After executing *update*:\n",
      "keys == dict_keys(['weights', 'biases', 'nlayers'])\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5  # Choice of learning rate\n",
    "model = update(eta, model)  # the dict model is updated *again* by calling *update*\n",
    "print(f'After executing *update*:\\nkeys == {model.keys()}')\n",
    "# Observe fewer dict keys: extraneous keys have been freed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the required sequence of executions: (forward -> backward -> update -> forward -> backward -> ...)\n",
    "# If done out of sequence, results in KeyError\n",
    "backward(y, model)  # This should cause an exception (KeyError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement steepest descent in a loop for random training data\n",
    "\n",
    "Let's now attempt to use our NumPy-based model to implement the steepest descent algorithm. We'll explain these numbers shortly in the context of the MNIST digit classification problem.\n",
    "\n",
    "+ Generate random arrays `X` and `y` of dimensions $28^2 \\times N_{\\mathrm{batch}}$ and $10\\times N_{\\mathrm{batch}}$ respectively where $N_{\\mathrm{batch}}=10$.\n",
    "+ Initialize the network architecture using `initialize_model` as above to require an input layer of $28^2$ units, a hidden layer of 15 units, and an output layer of 10 units.\n",
    "+ Choose a learning rate of, say, $\\eta=0.5$ and a number of epochs `n_epoch` of, say, $30$.\n",
    "+ Construct a for loop with `n_epochs` iterations in which:\n",
    "    + The output `yhat` is computed from the input`X` using `forward`.\n",
    "    + The function `backward` is called to compute the gradients of the loss function with respect to the weights and biases.\n",
    "    + Update the network parameters using the function `update`.\n",
    "    + Compute and print out the epoch (iteration counter) and the value of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss: 64.21327609189171\n",
      "Epoch: 1\tLoss: 47.34379050842908\n",
      "Epoch: 2\tLoss: 43.71687139792642\n",
      "Epoch: 3\tLoss: 41.78607462242981\n",
      "Epoch: 4\tLoss: 40.969502913546926\n",
      "Epoch: 5\tLoss: 40.26610818531366\n",
      "Epoch: 6\tLoss: 39.76748908880407\n",
      "Epoch: 7\tLoss: 39.359176658443346\n",
      "Epoch: 8\tLoss: 39.0410062005297\n",
      "Epoch: 9\tLoss: 38.796593428679195\n",
      "Epoch: 10\tLoss: 38.598957210219105\n",
      "Epoch: 11\tLoss: 38.39237694818983\n",
      "Epoch: 12\tLoss: 38.2447266573542\n",
      "Epoch: 13\tLoss: 38.11766770953976\n",
      "Epoch: 14\tLoss: 38.004353249958484\n",
      "Epoch: 15\tLoss: 37.89999750533945\n",
      "Epoch: 16\tLoss: 37.789157199398225\n",
      "Epoch: 17\tLoss: 37.79039394136049\n",
      "Epoch: 18\tLoss: 37.61937538146205\n",
      "Epoch: 19\tLoss: 37.59997945699664\n",
      "Epoch: 20\tLoss: 37.46913220685245\n",
      "Epoch: 21\tLoss: 37.32321909769307\n",
      "Epoch: 22\tLoss: 37.45031696477242\n",
      "Epoch: 23\tLoss: 37.12975468555189\n",
      "Epoch: 24\tLoss: 36.939170509981714\n",
      "Epoch: 25\tLoss: 36.76378046681027\n",
      "Epoch: 26\tLoss: 36.641533178619056\n",
      "Epoch: 27\tLoss: 36.51578607803842\n",
      "Epoch: 28\tLoss: 36.39805737380324\n",
      "Epoch: 29\tLoss: 36.284711843974854\n"
     ]
    }
   ],
   "source": [
    "N_batch = 10\n",
    "n_epochs = 30\n",
    "dimensions = [784, 15, 10]\n",
    "X = np.random.randn(dimensions[0], N_batch)\n",
    "y = np.random.randn(dimensions[-1],N_batch)\n",
    "eta = 0.5\n",
    "model = initialize_model(dimensions)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat, model = forward(X, model)\n",
    "    err = loss(yhat, y)\n",
    "    print(f'Epoch: {epoch}\\tLoss: {err}')\n",
    "    # Fill in: compute the derivatives by backpropagation\n",
    "    # Fill in: update the weights & biases\n",
    "    model = backward(y, model)\n",
    "    update(eta, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modify the steepest descent loop to make a plot\n",
    "\n",
    "Let's alter the preceding loop to accumulate selected epoch & loss values in lists for plotting.\n",
    "\n",
    "+ Set `N_batch` and `n_epochs` to be larger, say, $50$ and $30,000$ respectively.\n",
    "+ Change the preceding `for` loop so that:\n",
    "    + The `epoch` counter and the loss value are accumulated into lists every, say, `SKIP` iterations where `SKIP==500`.\n",
    "    + Eliminate the `print` statement(s) to save on output.\n",
    "+ After the `for` loop terminates, make a `semilogy` plot to verify that the loss function is actually decreasing with sucessive epochs.\n",
    "    + Use the list `epochs` to accumulate the `epoch` every 500 epochs.\n",
    "    + Use the list `losses` to accumulate the values of the loss function every 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_batch = 50\n",
    "n_epochs = 2000\n",
    "SKIP = 50\n",
    "dimensions = [784, 15, 10]\n",
    "X = np.random.randn(dimensions[0], N_batch)\n",
    "y = np.random.randn(dimensions[-1],N_batch)\n",
    "eta = 0.5\n",
    "model = initialize_model(dimensions)\n",
    "\n",
    "# accumulate the epoch and loss in these respective lists every SKIP epochs\n",
    "epochs, losses = [], []\n",
    "for epoch in range(n_epochs):\n",
    "    yhat, model = forward(X, model)\n",
    "    model = backward(y, model)\n",
    "    update(eta, model)\n",
    "    if epoch%SKIP == 0:\n",
    "        err = loss(yhat, y)\n",
    "        epochs.append(epoch)\n",
    "        losses.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# code for plotting once that the lists epochs and losses are accumulated\n",
    "fig = plt.figure(); ax = fig.add_subplot(111)\n",
    "ax.set_xlim([0,n_epochs]); ax.set_ylim([min(losses), max(losses)]);\n",
    "ax.set_xticks(epochs[::5]); ax.set_xlabel(\"Epochs\"); ax.grid(True);\n",
    "ax.set_ylabel(r'$\\mathcal{E}$'); \n",
    "h1 = ax.semilogy(epochs, losses, 'r-', label=r'$\\mathcal{E}$')\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
